<meta charset="utf-8"/>
<h3>
 Question 1
</h3>
<co-content>
 <p>
  In the videos, we described using either supervised learning or a prompt-based development process to build a restaurant review sentiment classifier. Which of the following statements about prompt-based development is correct?
 </p>
</co-content>
<form>
 <label>
  <input name="0" type="radio"/>
  <co-content>
   <span>
    Prompt-based development is generally much faster than supervised learning.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="radio"/>
  <co-content>
   <span>
    Prompt-based development requires that you collect hundreds or thousands of labeled examples.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="radio"/>
  <co-content>
   <span>
    Prompt-based development requires that you collect hundreds or thousands of unlabeled examples (meaning reviews without a label B to say if it is positive or negative sentiment).
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="radio"/>
  <co-content>
   <span>
    If you want to classify reviews as positive, neutral, or negative (3 possible outputs) there is no way to write a prompt to do so: An LLM can generate only 2 outputs.
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 2
</h3>
<co-content>
 <p>
  What is a token in the context of a large language model (LLM)?
 </p>
</co-content>
<form>
 <label>
  <input name="1" type="radio"/>
  <co-content>
   <span>
    The part of the LLM output that has primarily symbolic rather than substantive value (as in, “the court issued a token fine”, or “the LLM generated a token output”).
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="1" type="radio"/>
  <co-content>
   <span>
    A physical device or digital code to authenticate a user's identity.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="1" type="radio"/>
  <co-content>
   <span>
    A word or part of a word in either the input prompt or LLM output.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="1" type="radio"/>
  <co-content>
   <span>
    A unit of cryptocurrency (like bitcoin or other “crypto tokens”) that you can use to pay for LLM services.
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 3
</h3>
<co-content>
 <p>
  What are the major steps of the lifecycle of a Generative AI project?
 </p>
</co-content>
<form>
 <label>
  <input name="2" type="checkbox"/>
  <co-content>
   <span>
    Scope project → Build/improve system → Internal evaluation → Deploy and monitor
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="2" type="checkbox"/>
  <co-content>
   <span>
    Scope project → Internal evaluation → Build/improve system → Deploy and monitor
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="2" type="checkbox"/>
  <co-content>
   <span>
    Scope project → Internal evaluation → Deploy and monitor → Build/improve system
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="2" type="checkbox"/>
  <co-content>
   <span>
    Scope project → Build/improve system → Deploy and monitor → Internal evaluation
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 4
</h3>
<co-content>
 <p>
  You are building a customer service chatbot. Why is it important to monitor the performance of the system after it is deployed?
 </p>
</co-content>
<form>
 <label>
  <input name="3" type="radio"/>
  <co-content>
   <span>
    Because of the LLM’s knowledge cutoff, we must continuously monitor the knowledge cutoff and update its knowledge frequently.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="3" type="radio"/>
  <co-content>
   <span>
    This is false. So long as internal evaluation is done well, further monitoring is not necessary.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="3" type="radio"/>
  <co-content>
   <span>
    In case customers say something that causes the chatbot to respond in an unexpected way, monitoring lets you discover problems and fix them.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="3" type="radio"/>
  <co-content>
   <span>
    Every product should be monitored to track customer satisfaction -- this is good practice for all software.
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 5
</h3>
<co-content>
 <p>
  You are working on using an LLM to summarize research reports. Suppose an average report contains roughly 6,000 words. Approximately how many tokens would it take an LLM to process 6,000 input words? (Assume 1 token = 3/4 words, or equivalently, 1 word \approx 1.333 tokens).
 </p>
</co-content>
<form>
 <label>
  <input name="4" type="radio"/>
  <co-content>
   <span>
    4,500 tokens (6000 * 3/4)
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="4" type="radio"/>
  <co-content>
   <span>
    6,000 tokens
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="4" type="radio"/>
  <co-content>
   <span>
    8,000 tokens (about 6000 * 1.333)
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="4" type="radio"/>
  <co-content>
   <span>
    14,000 tokens (about 6000 * 1.333   the original 6000 words)
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
